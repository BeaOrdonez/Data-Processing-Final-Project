# -*- coding: utf-8 -*-
"""DATA PROCESSING FINAL PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O4Yckl71DMYDzYRTO0pFZiRGP8SjdSg4

**Authors:
Aïssétou Sogoba, Beatriz Ordoñez, Marta Montalbán**

### **1.ANALYSIS OF INPUT VARIABLES**

**Loading data and preparing data**
"""

import pandas as pd
import numpy as np
from termcolor import colored
import seaborn as sns
import matplotlib.pyplot as plt

# Load the JSON file into a DataFrame
import json
with open('full_format_recipes_def.json', 'r') as f:
    datos = json.load(f)
df = pd.DataFrame(datos)

"""**Eliminate null lines in dataset**"""

# check if some lines contain NaN values for ratings, desc or directions
contains_nan = np.isnan(df['rating']).any()
df = df.dropna(subset=['rating', 'desc', 'directions'])

##Check if there are still in directions column for example
nan_indices = df[df['directions'].isna()].index
rows_with_nan = df.iloc[nan_indices]

"""**Saving the target column for later analysis**"""

dfRatings=df['rating']

"""**Analysis of "Categories"**"""

# Count the most common categories
category_counts = df['categories'].explode().value_counts()[:10]
top_categories = category_counts.index

df['categories'] = df['categories'].apply(lambda x: x if isinstance(x, list) else [])
filtered_data = df[df['categories'].apply(lambda x: any(cat in top_categories for cat in x))]

# Create a category-rating relationship
category_rating = filtered_data.explode('categories')
category_rating = category_rating[category_rating['categories'].isin(top_categories)]

# Plot average ratings per category
plt.figure(figsize=(12, 6))
sns.boxplot(x='categories', y='rating', data=category_rating)
plt.xticks(rotation=45)
plt.title('Average Ratings per Category')
plt.xlabel('Category')
plt.ylabel('Rating')
plt.legend()
plt.show()

"""**Correlation analysis**"""

# Analyze the correlation between numerical elements such as fat, calories... and rating
df_numeric = df.select_dtypes(include=['float64'])    # select numerical values between the input features of each recipe

#Compute the correlation matrix
correlation_matrix = df_numeric.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title('Correlation Matrix')
plt.show()

"""### **2. IMPLEMENTATION OF A PIPELINE FOR TEXT PROCESSING**

*Preprocessing functions*

---
"""

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# 1. Text preprocessing function
def preprocess_text(text):
    if isinstance(text, str):
      text = text.lower()                                           # convert the text to non capital letters
      text = re.sub(r'[^a-zA-Z0-9\s]', '', text)                    # remove special characters

    tokens = word_tokenize(text)                                    # Tokenization
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]    # remove stop words which are not interesting for our NLP
    return ' '.join(tokens)

# 2. Delete the "None" lines in the description columns
def handle_desc_column(value):
    if isinstance(value, list):
        value = ' '.join(value)
    # If the value is None or NaN
    if value is None:
        value = ''
    return str(value)

"""*Apply to our text*

---

> *1. Preprocess "Desc" column*
"""

df['processed_desc'] = df['desc'].apply(lambda x: preprocess_text(handle_desc_column(x)))

from gensim.models.phrases import Phrases
from gensim.corpora import Dictionary

def tokenize(text_name):

    mycorpus = df[text_column].dropna().tolist()
    mycorpus = [word_tokenize(desc) for desc in mycorpus]

    # Dictionnary creation
    mycorpus_token = []
    for i in range(len(mycorpus)):
      mycorpus_i = [el.strip().split() for el in mycorpus[i]]
      mycorpus_token.append(mycorpus_i)

    # Dictionary of tokens
    D = Dictionary(mycorpus)
    n_tokens = len(D)

    print('The dictionary contains', n_tokens, 'terms')
    print('First terms in the dictionary:')
    for n in range(10):
        print(str(n), ':', D[n])

    # Determine the number of items in the created dictionary
    no_below = 4           #Minimum number of recipes to keep a term in the dictionary
    no_above = .80         #Maximum proportion of recipes in which a term can appear to be kept in the dictionary

    D.filter_extremes(no_below=no_below,no_above=no_above)
    n_tokens = len(D)
    print('The dictionary contains', n_tokens, 'terms')

    return mycorpus, D

# Apply the tokenize function our input descriptions

text_column = 'processed_desc'
mycorpus, D = tokenize(text_column)

"""

> *2. Identify the number of descriptions in which each token appears*"""

#dataframe with 2 columns: token and ndesc, corresponding to the text of each token and the number of description where the token appears

def dataframe(dico):

  # dico is the dictionnary produced by the tokenization function
  df = pd.DataFrame([[token, D.dfs[idx]] for idx,token in D.items()], columns=['token', 'ndesc'])
  df.sort_values(by=['ndesc'], ascending=False, inplace=True, ignore_index=True)
  print('Number of tokens appearing in just one line :', sum(df['ndesc']==1))
  df = df[df['ndesc']>1]
  print(df)
  plt.hist(np.log10(df.ndesc.tolist()), bins=30)
  plt.show()

dataframe(D)

"""> *3. Bag of words representation of the corpus*"""

mycorpus_bow = [D.doc2bow(desc) for desc in mycorpus]       # transforms token list to lits of (token_id, n= number of occurence of token)

# Check for row 1000
print("Recipe", ' '.join(mycorpus[10]))
print("Sparse vector representation", mycorpus_bow[1000])
print("Word counts",list(map(lambda x: (D[x[0]], x[1]), mycorpus_bow[1000])))

"""### **3. VECTOR REPRESENTATION OF THE DOCUMENTS USING THREE DIFFERENT PROCEDURES**

*TD-IDF, Word2Vec model and with transformers*

---

**A. TF-IDF**
"""

from gensim.models import TfidfModel

tfidf = TfidfModel(mycorpus_bow)              # Prepare the module TD-IDF with the corpus
mycorpus_tfidf = tfidf[mycorpus_bow]          # Apply it to the corpus

## Probamos para la fila 1000
n_project = 1000
TFidf = tfidf[mycorpus_bow[n_project]]

"""**B. WORD2VEC**"""

from gensim.models import Word2Vec
import numpy as np

# We use the tokenized elements stored in mycorpus for Word2Vec

# Create the model
w2v_model = Word2Vec(sentences= mycorpus, vector_size=100, window=5, min_count= 1, workers=4)

# Represent descriptions as average of word embeddings
def average_word2vec(tokens, model):
    vectors = [model.wv[word] for word in tokens if word in model.wv]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(model.vector_size)  # when the desc contains some words that are not in the vocab of the word2Vec

X_w2v = np.vstack([average_word2vec(desc, w2v_model) for desc in mycorpus])

print(X_w2v)

"""**C. TRANSFORMERS**

> *1. Importing necessary things*
"""

!pip install datasets

!pip install torch

"""*2. Transformer configuration*"""

from transformers import RobertaTokenizer, RobertaModel
import torch
from datasets import load_dataset

# Cargar el dataset with Hugging Face
dataset = load_dataset("json", data_files="full_format_recipes_def.json", split='train')
dataset_desc = dataset.map(lambda example: {'desc': example['desc']}, remove_columns=[col for i, col in enumerate(dataset.column_names) if i != 5])

# Cargar el tokenizer y el modelo RoBERTa preentrenado
tokenizer = RobertaTokenizer.from_pretrained("roberta-base" )
model = RobertaModel.from_pretrained("roberta-base")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

#Function to obtain the embeddings of the texts
def get_roberta_embeddings(texts):

    # Tokenize texts
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512).to(device)
    inputs = inputs.to(device)

    # Obtain outputs of RoBERTa model
    with torch.no_grad():
        outputs = model(**inputs)

    # Save embeddings of last layer
    embeddings = outputs.last_hidden_state.mean(dim=1)

    return embeddings.cpu().numpy()

removed_indices = []

def process_desc_column(batch):

    desc_texts = [desc for desc in batch['desc'] if desc is not None and desc != ""]
    if desc_texts:
        embeddings = get_roberta_embeddings(desc_texts)                                # Process all descriptions in a batch
        return {'desc_embeddings': [embed.squeeze().numpy() for embed in embeddings]}  # Return numpy arrays
    else:
        return {'desc_embeddings': []}                                                 # Return empty list for invalid rows

def filter_and_store_removed_indices(example, idx):
    if example['desc'] is None or example['desc'] == "" or example['rating'] is None:
        removed_indices.append(idx)
        return False
    return True

"""*3. Apply to our data*"""

# filter invalid lines
filtered_dataset = dataset.filter(filter_and_store_removed_indices, with_indices= True)

# Generate and save embeddings
batch_size = 16
all_embeddings = []

for i in range(0, len(filtered_dataset), batch_size):
    batch = filtered_dataset[i : i + batch_size]
    desc_texts = batch["desc"]  # Extract descriptions
    embeddings = get_roberta_embeddings(desc_texts)
    all_embeddings.extend(embeddings)


# Convert to a NumPy array and save
all_embeddings = np.array(all_embeddings)
np.save("desc_embeddings.npy", all_embeddings)
print(f"Saved embeddings to 'desc_embeddings.npy'")

"""*4. Use of PCA for dimensionality reduction*"""

from sklearn.decomposition import PCA

pca = PCA(n_components=100)  # Reduce to n_components dimension
reduced_embeddings = pca.fit_transform(all_embeddings)

# Save the reduced embeddings
np.save("desc_embeddings_pca.npy", reduced_embeddings)
print(f"Saved reduced embeddings to 'desc_embeddings_pca.npy'")

# Save PCA components
np.save("pca_components.npy", pca.components_)

"""*5. Load saved embeddings*"""

loaded_embeddings_no_pca = np.load("desc_embeddings.npy")     # without use of PCA

loaded_embeddings = np.load("desc_embeddings_pca.npy")        # for reduced one after use of PCA
print(f"Loaded embeddings shape: {loaded_embeddings.shape}")

ratings = np.array(filtered_dataset["rating"])  # Convert to a NumPy array

"""### **4. TRAINING AND EVALUATION OF REGRESSION MODELS**

##**Random Forest**
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

def Random_forest(X,y):

    # split data into training and testing data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


    # Train Random Forest
    rf_model = RandomForestRegressor()
    rf_model.fit(X_train, y_train)

    # 9. Hacer predicciones
    y_pred = rf_model.predict(X_test)

    # 10. Evaluar el modelo
    mse = mean_squared_error(y_test, y_pred)
    r_score = r2_score(y_test , y_pred)
    return(f'Mean Squared Error: {mse}, R² Score:{r_score}')

"""*Prepare "Rating" column, apply corresponding filter for model evaluation*"""

# delete rows where directions or descriptions or ratings is a NaN
contains_nan = np.isnan(df['rating']).any()
df = df.dropna(subset=['rating', 'desc', 'directions'])

# Find rows where rating is NaN to check the suppression
nan_indices = df[df['rating'].isna()].index

# Retrieve rows with NaN in the rating column in case
rows_with_nan = df.iloc[nan_indices]

"""*1. Transformers evaluation with Random forest*

---


"""

Random_forest(loaded_embeddings,df['rating'])

"""*2. Word2vec evaluation with Random forest*

---


"""

Random_forest(X_w2v, df['rating'])

"""*Convert the TD-IDF vectors into a sparse matrix for evaluation*


"""

from gensim.matutils import corpus2csc
from scipy.sparse import csr_matrix

sparse_matrix = corpus2csc(mycorpus_tfidf).T  # Transpose to match (n_samples, n_features)
sparse_matrix = csr_matrix(sparse_matrix)

print("Shape of mycorpus_tfidf:", sparse_matrix.shape)
print("Shape of df['rating']:", len(df['rating']))

"""*TF IDF evaluation with Random forest*

---


"""

Random_forest(sparse_matrix, df['rating'])

"""##**Neural Netwok**"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Create a PyTorch model
class RegressionNN(nn.Module):
    def __init__(self, input_dim):
        super(RegressionNN, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.fc(x)

def NN_eval(X, y):

    # Data splitting into train and test dataset
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Convert to tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)    # add a dimension for regression

    # Create DataLoader
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    # Initialize model, loss, and optimizer
    model = RegressionNN(input_dim=X_train.shape[1])
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Training loop over number of epochs
    for epoch in range(5):                    # Training for 5 epochs
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            loss.backward()
            optimizer.step()

    # Model evaluation
    model.eval()
    with torch.no_grad():
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)
        predictions = model(X_test_tensor)
        test_loss = criterion(predictions, y_test_tensor)
        print(f"Test Loss: {test_loss.item()}")

        predictions_np = predictions.numpy()   # convert predictions to NumPy array
        r2 = r2_score(y_test, predictions_np)  # compute R^2 score
        print(f"R^2 Score: {r2}")

"""*1. Evaluation of Word2vec with Neural Networks*

---


"""

NN_eval(X_w2v, dfRatings.to_numpy())

"""*2. Evaluation of TF - IDF with Neural networks*

---


"""

NN_eval(sparse_matrix.toarray(), dfRatings.to_numpy())

"""*3. Evaluation of transformers with Neural networks*

---


"""

NN_eval(loaded_embeddings, df['rating'])

"""##**Comparison**"""

import matplotlib.pyplot as plt

# Nombres de los modelos y R^2 scores
models = ["TF-IDF", "Word2vec", "Transformes"]
r2_scores = [0.18063449140055166, 0.1713664051413416, 0.04609273692251947]

# Crear el gráfico de barras
plt.figure(figsize=(8, 5))  # Tamaño del gráfico
plt.bar(models, r2_scores, color='skyblue', edgecolor='black')

# Etiquetas y título
plt.xlabel("Modelos", fontsize=12)
plt.ylabel("R² Score", fontsize=12)
plt.title("Comparación de R² Score entre Modelos", fontsize=14)
plt.ylim(0, 1)
plt.grid(axis="y", linestyle="--", alpha=0.7)

# Mostrar el gráfico
plt.show()

"""### **Fine-tuning a pre-trained model from Hugging Face**"""

dataset = load_dataset("json", data_files="full_format_recipes_def.json", split='train')

removed_indices = []
def filter_and_store_removed_indices(example, idx):
    if not example['desc'] or example['desc'].strip() == "" or example['rating'] is None:
        removed_indices.append(idx)  # Registrar el índice de la fila eliminada
        return False  # Filtrar esta fila
    return True  # Conservar esta fila                                            # Mantener esta fila

from sklearn.model_selection import train_test_split

filtered_dataset = dataset.filter(filter_and_store_removed_indices, with_indices= True)

## We generate test and train sets for future evaluation

train_test_split_result = filtered_dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split_result['train']
eval_dataset = train_test_split_result['test']

print(filtered_dataset.shape)
print(filtered_dataset.column_names)

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("roberta-base")

def tokenize_batch(batch):
    tokenized_batch = tokenizer(batch["desc"], padding= 'max_length', truncation=True, return_tensors="pt", max_length=246)
    return tokenized_batch

tokenized_dataset = train_dataset.map(tokenize_batch, batched=True)
#print(tokenized_dataset.shape)
#print(tokenized_dataset[0])

"""Check the lengths after tokenization"""

print(len(tokenized_dataset['input_ids'][0]))
print(len(tokenized_dataset['attention_mask'][0]))
lengths =[len(seq) for seq in tokenized_dataset['input_ids']]
print(lengths)
if any(length != 246 for length in lengths):

    print("Hay secuencias con longitud distinta a 246")
else:
    print("Todas las secuencias tienen longitud 246")

from transformers import AutoModelForSequenceClassification

# Cargar el modelo preentrenado con la cabeza de regresión (un solo valor de salida)
model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=1)

# Mover el modelo al dispositivo adecuado (GPU si está disponible)
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

input_ids_tensor = torch.tensor(tokenized_dataset['input_ids'])
attention_mask_tensor = torch.tensor(tokenized_dataset['attention_mask'])
labels_tensor = torch.tensor(tokenized_dataset['rating'])
print(input_ids_tensor.shape, attention_mask_tensor.shape, labels_tensor.shape)

import torch
from torch.optim import AdamW
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

batch_size = 16
dataset = TensorDataset(input_ids_tensor, attention_mask_tensor, labels_tensor)
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)


optimizer = AdamW(model.parameters(), lr=2e-5)
criterion = nn.MSELoss()

# Train the model
for epoch in range(3):
    model.train()
    epoch_loss = 0.0

    for batch in data_loader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device).float()

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = outputs.logits.squeeze()

        loss = criterion(predictions, labels)
        epoch_loss += loss.item()

        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}, Average Loss: {epoch_loss / len(data_loader)}")

model.save_pretrained("./trained_model")
tokenizer.save_pretrained("./trained_model")

"""*Apply trained model to our data, using test dataset*"""

tokenized_datasetTEST = eval_dataset.map(tokenize_batch, batched=True)

"""Create tensors of tokenized test dataset"""

input_ids_tensorTEST = torch.tensor(tokenized_datasetTEST['input_ids'])
attention_mask_tensorTEST = torch.tensor(tokenized_datasetTEST['attention_mask'])
labels_tensorTEST = torch.tensor(tokenized_datasetTEST['rating'])
print(input_ids_tensorTEST.shape, attention_mask_tensorTEST.shape, labels_tensorTEST.shape)

"""Apply model"""

batch_size = 16
test_dataset = TensorDataset(input_ids_tensorTEST, attention_mask_tensorTEST, labels_tensorTEST)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

model.eval()
total_loss = 0.0
predictions = []
true_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device).float()

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits.squeeze()

        predictions.extend(logits.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

        loss = criterion(logits, labels)
        total_loss += loss.item()

average_loss = total_loss / len(test_loader)
print(f"Pérdida promedio en el conjunto de prueba: {average_loss}")

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

mse = mean_squared_error(true_labels, predictions)
mae = mean_absolute_error(true_labels, predictions)
r2 = r2_score(true_labels, predictions)

print(f"MSE: {mse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"R²: {r2:.4f}")

"""##**Project extension**"""

# vectorization using BPE with tokenizer

from tokenizers import Tokenizer, models, trainers, pre_tokenizers
from tokenizers.trainers import BpeTrainer

descriptions = df['desc'].dropna().tolist()  # Drop any null values

# Combine the list of descriptions into a single list of sentences
texts = [desc for desc in descriptions if isinstance(desc, str)]

tokenizer = Tokenizer(models.BPE())

# Pre-tokenizer splits text into words and subwords
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

# we define the trainer with a given vocab size and some special tokens
trainer = BpeTrainer(vocab_size=5000, min_frequency=2, special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]"])

# Training of the tokenizer on the list of recipe descriptions
tokenizer.train_from_iterator(texts, trainer)

# Then we save the tokenizer for later use
tokenizer.save("bpe_tokenizer.json")
print("Tokenizer trained and saved as 'bpe_tokenizer.json'")

from sklearn.preprocessing import MultiLabelBinarizer

tokenizer = Tokenizer.from_file("bpe_tokenizer.json")

# Encode all descriptions
def encode_descriptions(texts):
    return [tokenizer.encode(text).ids for text in texts]       # Convert each description to token IDs using the tokenizer

encoded_descriptions = encode_descriptions(descriptions)

# Since RandomForest doesn't support variable-length inputs, pad sequences to the same length
max_len = max(len(seq) for seq in encoded_descriptions)       # Determine the maximum sequence length


padded_descriptions = np.array([seq + [0] * (max_len - len(seq)) for seq in encoded_descriptions])   # Pad all sequences to the maximum length

print(f"Padded encoded descriptions shape: {padded_descriptions.shape}")

Random_forest(padded_descriptions, df['rating'])

"""##**Analysis with three features combined: directions, descriptions, categories**"""

# process the directions
df['processed_directions'] = df['directions'].apply(lambda x: preprocess_text(handle_desc_column(x)))

# put each processed data as a string again for joining later

df['processed_directions'] = df['processed_directions'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)
df['processed_desc'] = df['processed_desc'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)
df['categories'] = df['categories'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))

# Join them into a unique feature
df['processed_directions_desc_categories'] = (df['processed_desc'] + ' ' + df['processed_directions'] + ' ' + df['categories'])

# check if some lines contain NaN values for ratings, desc or directions and remove them

df = df.dropna(subset=['rating', 'desc', 'directions'])

text_column_3_variables = 'processed_directions_desc_categories'

# tokenize this input data
mycorpus_2, D_2 = tokenize(text_column_3_variables)

# create the model for word2vec
w2v_model_2 = Word2Vec(sentences = mycorpus_2, vector_size=100, window=5, min_count= 1, workers=4)

# get the vectors
X_w2v_2 = np.vstack([average_word2vec(data, w2v_model) for data in mycorpus_2])

# evaluation of word2vec with three features
Random_forest(X_w2v_2, df['rating'])

# TD-IDF
mycorpus_bow_2 = [D.doc2bow(desc) for desc in mycorpus_2]
from gensim.models import TfidfModel
tfidf_2 = TfidfModel(mycorpus_bow_2)              # to prepare the module TD-IDF with the corpus
mycorpus_tfidf_2 = tfidf_2[mycorpus_bow_2]          # apply it to the corpus

TFidf_2 = tfidf_2[mycorpus_bow_2[n_project]]

from gensim.matutils import corpus2csc
from scipy.sparse import csr_matrix

# Convert TransformedCorpus to a SciPy sparse matrix
sparse_matrix_2 = corpus2csc(mycorpus_tfidf_2).T  # Transpose to match (n_samples, n_features)

# Ensure it is in CSR format
sparse_matrix_2 = csr_matrix(sparse_matrix_2)

Random_forest(sparse_matrix_2, df['rating'])